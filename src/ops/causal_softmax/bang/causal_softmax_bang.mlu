#include "../../../devices/bang/common_bang.h"
#include "bang.h"
#include "bang_device_functions.h"
#include "causal_softmax_bang.h"
#include "cnrt.h"

const int SRC_MAX_SIZE = 1024 * 64;//至少大于等于128字节
__nram__ char nram_buffer[NRAM_MAX_SIZE];

template<typename T>
__mlu_device__ void causal_softmaxKernel(T *destination, int *strideDest, int *shape, int othersize, int dimsize, int dimS, int mask, int ndim) {

    const int maxNum = SRC_MAX_SIZE / sizeof(T);
    int wSize = 128 / sizeof(T);
    __nram__ T srcMax[2];
    if (dimsize > maxNum) {
        T *src = (T *) nram_buffer;        //[maxNum]
        T *destSum = src + maxNum;         //[maxNum]
        T *destSumFinal = destSum + maxNum;//[wSize]
        T *tmp = destSumFinal + wSize;     //[maxNum]

        T destOldMax;
        T destNewMax;

        int remain = dimsize % maxNum;
        int repeat = (dimsize - remain) / maxNum;

        int remainT = othersize % taskDim;
        int stepEasy = (othersize - remainT) / taskDim;
        int stepHard = stepEasy + 1;
        int step = (taskId < remainT ? stepHard : stepEasy);
        int indStart = (taskId < remainT ? taskId * stepHard : (taskId - remainT) * stepEasy + remainT * stepHard);

        for (int i = indStart; i < indStart + step; i++) {
            int indd = 0;
            int indi = i;
            int lastI = indi % shape[ndim - 2];
            for (int j = ndim - 2; j >= 0; --j) {

                indd += (indi % shape[j]) * strideDest[j];
                indi /= shape[j];
            }

            if (mask + 1 + lastI < maxNum) {
                __bang_write_value(src, maxNum, -INFINITY);                                   //提前设置负无穷
                __memcpy(src, destination + indd, (mask + 1 + lastI) * sizeof(T), GDRAM2NRAM);//从destination读取对应数据
                __bang_argmax(srcMax, src, maxNum);                                           //获取最大值
                __bang_write_value(destSum, maxNum, srcMax[0]);
                __memcpy(destSum, src, (mask + 1 + lastI) * sizeof(T), NRAM2NRAM);//destSum前面(mask + 1 + lastI)为src，后面部分为最大值
                __bang_sub_scalar(destSum, destSum, srcMax[0], maxNum);           //destSum前面(mask + 1 + lastI)为(src - M)，后面部分为0
                __bang_active_exp_less_0(destSum, destSum, maxNum);               //destSum前面(mask + 1 + lastI)为exp(src - M)，后面部分为1
                __bang_write_zero(src, maxNum);                                   //重新设置src全部为0
                __memcpy(src, destSum, (mask + 1 + lastI) * sizeof(T), NRAM2NRAM);//src前面(mask + 1 + lastI)为exp(src - M)，后面部分为0

                if (maxNum >= wSize) {
                    int segNum = maxNum / wSize;//准备数值求和
                    for (int strip = segNum / 2; strip > 0; strip = strip / 2) {
                        for (int j = 0; j < strip; j++) {
                            __bang_add(destSum + j * wSize, destSum + j * wSize, destSum + (j + strip) * wSize, wSize);
                        }
                    }
                    __bang_reduce_sum(destSumFinal, destSum, wSize);//此时destSum[0]保存的就是当前maxNum长度数据的数值和

                } else {
                    __memcpy(destSumFinal, destSum, maxNum * sizeof(T), NRAM2NRAM);
                    __bang_reduce_sum(destSumFinal, destSumFinal, wSize);//此时destSum[0]保存的就是当前maxNum长度数据的数值和
                }
                T globalSumInv = 1.0 / (destSumFinal[0] - (maxNum - (mask + 1 + lastI)));//下面开始指数变换，写回GDRAM
                __bang_mul_scalar(src, src, globalSumInv, maxNum);

                __memcpy(destination + indd, src, maxNum * sizeof(T), NRAM2GDRAM);
                __bang_write_zero(src, maxNum);
                for (int s = 1; s < repeat; s++) {
                    __memcpy(destination + indd + s * maxNum, src, maxNum * sizeof(T), NRAM2GDRAM);
                }
                if (remain) {
                    __memcpy(destination + indd + repeat * maxNum, src, remain * sizeof(T), NRAM2GDRAM);
                }
            } else {
                int newRemain = (mask + 1 + lastI) % maxNum;
                int nR = (mask + 1 + lastI - newRemain) / maxNum;

                __bang_write_zero(destSum, maxNum);
                __bang_write_zero(destSumFinal, wSize);

                destOldMax = -INFINITY;
                destNewMax = -INFINITY;
                for (int s = 0; s < nR; s++) {

                    __memcpy(src, destination + indd + s * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                    __bang_argmax(srcMax, src, maxNum);

                    if (destNewMax < srcMax[0]) {
                        destNewMax = srcMax[0];
                    }
                    __bang_sub_scalar(src, src, destNewMax, maxNum);
                    __bang_active_exp_less_0(src, src, maxNum);

                    if (s > 0) {
                        __bang_mul_scalar(destSum, destSum, exp(destOldMax - destNewMax), maxNum);
                    }
                    __bang_add(destSum, destSum, src, maxNum);

                    destOldMax = destNewMax;
                }

                if (newRemain) {
                    //__bang_write_value(src, maxNum, -INFINITY);

                    __memcpy(src, destination + indd + nR * maxNum, newRemain * sizeof(T), GDRAM2NRAM);

                    __bang_argmax(srcMax, src, maxNum);

                    if (destNewMax < srcMax[0]) {
                        destNewMax = srcMax[0];
                    }

                    __bang_write_value(tmp, maxNum, destNewMax);
                    __memcpy(tmp, src, newRemain * sizeof(T), NRAM2NRAM);

                    __bang_sub_scalar(tmp, tmp, destNewMax, maxNum);
                    __bang_active_exp_less_0(tmp, tmp, maxNum);

                    if (nR > 0) {
                        __bang_mul_scalar(destSum, destSum, exp(destOldMax - destNewMax), maxNum);
                    }
                    __bang_add(destSum, destSum, tmp, maxNum);

                    destOldMax = destNewMax;
                }

                if (maxNum >= wSize) {
                    int segNum = maxNum / wSize;//准备数值求和
                    for (int strip = segNum / 2; strip > 0; strip = strip / 2) {
                        for (int j = 0; j < strip; j++) {
                            __bang_add(destSum + j * wSize, destSum + j * wSize, destSum + (j + strip) * wSize, wSize);
                        }
                    }
                    __bang_reduce_sum(destSumFinal, destSum, wSize);//此时destSum[0]保存的就是当前maxNum长度数据的数值和

                } else {

                    __memcpy(destSumFinal, destSum, maxNum * sizeof(T), NRAM2NRAM);
                    __bang_reduce_sum(destSumFinal, destSumFinal, wSize);//此时destSum[0]保存的就是当前maxNum长度数据的数值和
                }

                T globalSumInv;
                if (newRemain) {
                    globalSumInv = 1.0 / (destSumFinal[0] - (maxNum - newRemain));//下面开始指数变换，写回GDRAM

                } else {
                    globalSumInv = 1.0 / destSumFinal[0];//下面开始指数变换，写回GDRAM
                }

                for (int s = 0; s < nR; s++) {
                    __memcpy(src, destination + indd + s * maxNum, maxNum * sizeof(T), GDRAM2NRAM);

                    __bang_sub_scalar(src, src, destNewMax, maxNum);
                    __bang_active_exp_less_0(src, src, maxNum);
                    __bang_mul_scalar(src, src, globalSumInv, maxNum);

                    __memcpy(destination + indd + s * maxNum, src, maxNum * sizeof(T), NRAM2GDRAM);
                }
                __bang_write_zero(src, maxNum);
                for (int s = nR; s < repeat; s++) {
                    __memcpy(destination + indd + s * maxNum, src, maxNum * sizeof(T), NRAM2GDRAM);
                }
                if (remain) {
                    __memcpy(destination + indd + repeat * maxNum, src, remain * sizeof(T), NRAM2GDRAM);
                }

                if (newRemain) {

                    __memcpy(src, destination + indd + nR * maxNum, newRemain * sizeof(T), GDRAM2NRAM);

                    __bang_sub_scalar(src, src, destNewMax, maxNum);
                    __bang_active_exp_less_0(src, src, maxNum);
                    __bang_mul_scalar(src, src, globalSumInv, maxNum);

                    __memcpy(destination + indd + nR * maxNum, src, newRemain * sizeof(T), NRAM2GDRAM);
                }
            }
        }
    } else {
        T *src = (T *) nram_buffer;      //[dimS]
        T *destSum = src + dimS;         //[dimS]
        T *destSumFinal = destSum + dimS;//[wSize]

        int remainT = othersize % taskDim;
        int stepEasy = (othersize - remainT) / taskDim;
        int stepHard = stepEasy + 1;
        int step = (taskId < remainT ? stepHard : stepEasy);
        int indStart = (taskId < remainT ? taskId * stepHard : (taskId - remainT) * stepEasy + remainT * stepHard);


        for (int i = indStart; i < indStart + step; i++) {

            int indd = 0;
            int indi = i;

            for (int j = ndim - 2; j >= 0; --j) {

                indd += (indi % shape[j]) * strideDest[j];
                indi /= shape[j];
            }
            __bang_write_value(src, dimS, -INFINITY);
            __bang_write_zero(destSumFinal, wSize);
            int lastI = i % shape[ndim - 2];
            __memcpy(src, destination + indd, (mask + 1 + lastI) * sizeof(T), GDRAM2NRAM);
            __bang_argmax(srcMax, src, dimS);
            __bang_write_value(destSum, dimS, srcMax[0]);
            __memcpy(destSum, src, (mask + 1 + lastI) * sizeof(T), NRAM2NRAM);
            __bang_sub_scalar(destSum, destSum, srcMax[0], dimS);
            __bang_active_exp_less_0(destSum, destSum, dimS);
            __bang_write_zero(src, dimS);
            __memcpy(src, destSum, (mask + 1 + lastI) * sizeof(T), NRAM2NRAM);
            int segNum = dimS / wSize;//准备数值求和
            for (int strip = segNum / 2; strip > 0; strip = strip / 2) {
                for (int j = 0; j < strip; j++) {
                    __bang_add(destSum + j * wSize, destSum + j * wSize, destSum + (j + strip) * wSize, wSize);
                }
            }
            __bang_reduce_sum(destSumFinal, destSum, wSize);                       //此时destSum[0]保存的就是当前maxNum长度数据的数值和
            T globalSumInv = 1.0 / (destSumFinal[0] - (dimS - (mask + 1 + lastI)));//下面开始指数变换，写回GDRAM
            __bang_mul_scalar(src, src, globalSumInv, dimS);

            __memcpy(destination + indd, src, dimsize * sizeof(T), NRAM2GDRAM);
        }
    }
}

template<typename T>
__mlu_global__ void causal_softmaxUnion1(T *destination, int *strideDest, int *shape, int othersize, int dimsize, int dimS, int mask, int ndim) {

    causal_softmaxKernel<T>(destination, strideDest, shape, othersize, dimsize, dimS, mask, ndim);
}

template<typename T>
void causal_softmax(cnrtQueue_t queue, void *destination, int *strideDest, int *shape, int othersize, int dimsize, int mask, int ndim) {
    int wSize = 128 / sizeof(T);
    auto y_ = reinterpret_cast<T *>(destination);

    int dimS;
    float mi = log2(dimsize);
    if (floor(mi) == mi) {
        dimS = dimsize;
    } else {
        dimS = pow(2, floor(mi) + 1);
    }
    if (dimS < wSize) {
        dimS = wSize;
    }

    cnrtDim3_t k_dim;
    cnrtFunctionType_t k_type;

    k_dim.x = 4;
    k_dim.y = 1;
    k_dim.z = 1;
    k_type = CNRT_FUNC_TYPE_UNION1;

    causal_softmaxUnion1<T><<<k_dim, k_type, queue>>>(y_, strideDest, shape, othersize, dimsize, dimS, mask, ndim);
    cnrtQueueSync(queue);
}

template<typename T>
__mlu_global__ void causal_softmaxDim_2(T *destination, int strideD_f, int othersize, int dimsize, int dimS, int mask) {

    const int maxNum = SRC_MAX_SIZE / sizeof(T);
    int wSize = 128 / sizeof(T);
    __nram__ T srcMax[2];
    if (dimsize > maxNum) {
        T *src = (T *) nram_buffer;        //[maxNum]
        T *destSum = src + maxNum;         //[maxNum]
        T *destSumFinal = destSum + maxNum;//[wSize]
        T *tmp = destSumFinal + wSize;     //[maxNum]

        T destOldMax;
        T destNewMax;

        int remain = dimsize % maxNum;
        int repeat = (dimsize - remain) / maxNum;

        int remainT = othersize % taskDim;
        int stepEasy = (othersize - remainT) / taskDim;
        int stepHard = stepEasy + 1;
        int step = (taskId < remainT ? stepHard : stepEasy);
        int indStart = (taskId < remainT ? taskId * stepHard : (taskId - remainT) * stepEasy + remainT * stepHard);

        for (int i = indStart; i < indStart + step; i++) {

            int indd = 0;
            int indi = i;
            int lastI = indi % othersize;

            indd += (indi % othersize) * strideD_f;

            if (mask + 1 + lastI < maxNum) {
                __bang_write_value(src, maxNum, -INFINITY);                                   //提前设置负无穷
                __memcpy(src, destination + indd, (mask + 1 + lastI) * sizeof(T), GDRAM2NRAM);//从destination读取对应数据
                __bang_argmax(srcMax, src, maxNum);                                           //获取最大值
                __bang_write_value(destSum, maxNum, srcMax[0]);
                __memcpy(destSum, src, (mask + 1 + lastI) * sizeof(T), NRAM2NRAM);//destSum前面(mask + 1 + lastI)为src，后面部分为最大值
                __bang_sub_scalar(destSum, destSum, srcMax[0], maxNum);           //destSum前面(mask + 1 + lastI)为(src - M)，后面部分为0
                __bang_active_exp_less_0(destSum, destSum, maxNum);               //destSum前面(mask + 1 + lastI)为exp(src - M)，后面部分为1
                __bang_write_zero(src, maxNum);                                   //重新设置src全部为0
                __memcpy(src, destSum, (mask + 1 + lastI) * sizeof(T), NRAM2NRAM);//src前面(mask + 1 + lastI)为exp(src - M)，后面部分为0

                if (maxNum >= wSize) {
                    int segNum = maxNum / wSize;//准备数值求和
                    for (int strip = segNum / 2; strip > 0; strip = strip / 2) {
                        for (int j = 0; j < strip; j++) {
                            __bang_add(destSum + j * wSize, destSum + j * wSize, destSum + (j + strip) * wSize, wSize);
                        }
                    }
                    __bang_reduce_sum(destSumFinal, destSum, wSize);//此时destSum[0]保存的就是当前maxNum长度数据的数值和

                } else {
                    __memcpy(destSumFinal, destSum, maxNum * sizeof(T), NRAM2NRAM);
                    __bang_reduce_sum(destSumFinal, destSumFinal, wSize);//此时destSum[0]保存的就是当前maxNum长度数据的数值和
                }
                T globalSumInv = 1.0 / (destSumFinal[0] - (maxNum - (mask + 1 + lastI)));//下面开始指数变换，写回GDRAM
                __bang_mul_scalar(src, src, globalSumInv, maxNum);

                __memcpy(destination + indd, src, maxNum * sizeof(T), NRAM2GDRAM);
                __bang_write_zero(src, maxNum);
                for (int s = 1; s < repeat; s++) {
                    __memcpy(destination + indd + s * maxNum, src, maxNum * sizeof(T), NRAM2GDRAM);
                }
                if (remain) {
                    __memcpy(destination + indd + repeat * maxNum, src, remain * sizeof(T), NRAM2GDRAM);
                }
            } else {
                int newRemain = (mask + 1 + lastI) % maxNum;
                int nR = (mask + 1 + lastI - newRemain) / maxNum;

                __bang_write_zero(destSum, maxNum);
                __bang_write_zero(destSumFinal, wSize);

                destOldMax = -INFINITY;
                destNewMax = -INFINITY;
                for (int s = 0; s < nR; s++) {

                    __memcpy(src, destination + indd + s * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                    __bang_argmax(srcMax, src, maxNum);

                    if (destNewMax < srcMax[0]) {
                        destNewMax = srcMax[0];
                    }
                    __bang_sub_scalar(src, src, destNewMax, maxNum);
                    __bang_active_exp_less_0(src, src, maxNum);

                    if (s > 0) {
                        __bang_mul_scalar(destSum, destSum, exp(destOldMax - destNewMax), maxNum);
                    }
                    __bang_add(destSum, destSum, src, maxNum);

                    destOldMax = destNewMax;
                }

                if (newRemain) {
                    //__bang_write_value(src, maxNum, -INFINITY);

                    __memcpy(src, destination + indd + nR * maxNum, newRemain * sizeof(T), GDRAM2NRAM);

                    __bang_argmax(srcMax, src, maxNum);

                    if (destNewMax < srcMax[0]) {
                        destNewMax = srcMax[0];
                    }

                    __bang_write_value(tmp, maxNum, destNewMax);
                    __memcpy(tmp, src, newRemain * sizeof(T), NRAM2NRAM);

                    __bang_sub_scalar(tmp, tmp, destNewMax, maxNum);
                    __bang_active_exp_less_0(tmp, tmp, maxNum);

                    if (nR > 0) {
                        __bang_mul_scalar(destSum, destSum, exp(destOldMax - destNewMax), maxNum);
                    }
                    __bang_add(destSum, destSum, tmp, maxNum);

                    destOldMax = destNewMax;
                }

                if (maxNum >= wSize) {
                    int segNum = maxNum / wSize;//准备数值求和
                    for (int strip = segNum / 2; strip > 0; strip = strip / 2) {
                        for (int j = 0; j < strip; j++) {
                            __bang_add(destSum + j * wSize, destSum + j * wSize, destSum + (j + strip) * wSize, wSize);
                        }
                    }
                    __bang_reduce_sum(destSumFinal, destSum, wSize);//此时destSum[0]保存的就是当前maxNum长度数据的数值和

                } else {

                    __memcpy(destSumFinal, destSum, maxNum * sizeof(T), NRAM2NRAM);
                    __bang_reduce_sum(destSumFinal, destSumFinal, wSize);//此时destSum[0]保存的就是当前maxNum长度数据的数值和
                }

                T globalSumInv;
                if (newRemain) {
                    globalSumInv = 1.0 / (destSumFinal[0] - (maxNum - newRemain));//下面开始指数变换，写回GDRAM

                } else {
                    globalSumInv = 1.0 / destSumFinal[0];//下面开始指数变换，写回GDRAM
                }

                for (int s = 0; s < nR; s++) {
                    __memcpy(src, destination + indd + s * maxNum, maxNum * sizeof(T), GDRAM2NRAM);

                    __bang_sub_scalar(src, src, destNewMax, maxNum);
                    __bang_active_exp_less_0(src, src, maxNum);
                    __bang_mul_scalar(src, src, globalSumInv, maxNum);

                    __memcpy(destination + indd + s * maxNum, src, maxNum * sizeof(T), NRAM2GDRAM);
                }
                __bang_write_zero(src, maxNum);
                for (int s = nR; s < repeat; s++) {
                    __memcpy(destination + indd + s * maxNum, src, maxNum * sizeof(T), NRAM2GDRAM);
                }
                if (remain) {
                    __memcpy(destination + indd + repeat * maxNum, src, remain * sizeof(T), NRAM2GDRAM);
                }

                if (newRemain) {

                    __memcpy(src, destination + indd + nR * maxNum, newRemain * sizeof(T), GDRAM2NRAM);

                    __bang_sub_scalar(src, src, destNewMax, maxNum);
                    __bang_active_exp_less_0(src, src, maxNum);
                    __bang_mul_scalar(src, src, globalSumInv, maxNum);

                    __memcpy(destination + indd + nR * maxNum, src, newRemain * sizeof(T), NRAM2GDRAM);
                }
            }
        }
    } else {
        T *src = (T *) nram_buffer;      //[dimS]
        T *destSum = src + dimS;         //[dimS]
        T *destSumFinal = destSum + dimS;//[wSize]

        int remainT = othersize % taskDim;
        int stepEasy = (othersize - remainT) / taskDim;
        int stepHard = stepEasy + 1;
        int step = (taskId < remainT ? stepHard : stepEasy);
        int indStart = (taskId < remainT ? taskId * stepHard : (taskId - remainT) * stepEasy + remainT * stepHard);


        for (int i = indStart; i < indStart + step; i++) {

            int indd = 0;
            int indi = i;


            indd += (indi % othersize) * strideD_f;
            __bang_write_value(src, dimS, -INFINITY);
            __bang_write_zero(destSumFinal, wSize);
            int lastI = i % othersize;
            __memcpy(src, destination + indd, (mask + 1 + lastI) * sizeof(T), GDRAM2NRAM);
            __bang_argmax(srcMax, src, dimS);
            __bang_write_value(destSum, dimS, srcMax[0]);
            __memcpy(destSum, src, (mask + 1 + lastI) * sizeof(T), NRAM2NRAM);
            __bang_sub_scalar(destSum, destSum, srcMax[0], dimS);
            __bang_active_exp_less_0(destSum, destSum, dimS);
            __bang_write_zero(src, dimS);
            __memcpy(src, destSum, (mask + 1 + lastI) * sizeof(T), NRAM2NRAM);
            int segNum = dimS / wSize;//准备数值求和
            for (int strip = segNum / 2; strip > 0; strip = strip / 2) {
                for (int j = 0; j < strip; j++) {
                    __bang_add(destSum + j * wSize, destSum + j * wSize, destSum + (j + strip) * wSize, wSize);
                }
            }
            __bang_reduce_sum(destSumFinal, destSum, wSize);                       //此时destSum[0]保存的就是当前maxNum长度数据的数值和
            T globalSumInv = 1.0 / (destSumFinal[0] - (dimS - (mask + 1 + lastI)));//下面开始指数变换，写回GDRAM
            __bang_mul_scalar(src, src, globalSumInv, dimS);

            __memcpy(destination + indd, src, dimsize * sizeof(T), NRAM2GDRAM);
        }
    }
}

template<typename T>
void causal_softmaxUnionDim_2(cnrtQueue_t queue, void *destination, int strideD_f, int othersize, int dimsize, int mask) {
    int wSize = 128 / sizeof(T);
    auto y_ = reinterpret_cast<T *>(destination);
    int dimS;
    float mi = log2(dimsize);
    if (floor(mi) == mi) {
        dimS = dimsize;
    } else {
        dimS = pow(2, floor(mi) + 1);
    }
    if (dimS < wSize) {
        dimS = wSize;
    }

    cnrtDim3_t k_dim;
    cnrtFunctionType_t k_type;

    k_dim.x = 4;
    k_dim.y = 1;
    k_dim.z = 1;
    k_type = CNRT_FUNC_TYPE_UNION1;

    causal_softmaxDim_2<T><<<k_dim, k_type, queue>>>(y_, strideD_f, othersize, dimsize, dimS, mask);
    cnrtQueueSync(queue);
}

template<typename T>
__mlu_global__ void causal_softmaxDim_3(T *destination, int strideD_f, int strideD_m, int othersize, int middle, int dimsize, int dimS, int mask) {

    const int maxNum = SRC_MAX_SIZE / sizeof(T);
    int wSize = 128 / sizeof(T);
    __nram__ T srcMax[2];
    int startDim = othersize / middle;
    if (dimsize > maxNum) {
        T *src = (T *) nram_buffer;        //[maxNum]
        T *destSum = src + maxNum;         //[maxNum]
        T *destSumFinal = destSum + maxNum;//[wSize]
        T *tmp = destSumFinal + wSize;     //[maxNum]

        T destOldMax;
        T destNewMax;

        int remain = dimsize % maxNum;
        int repeat = (dimsize - remain) / maxNum;

        int remainT = othersize % taskDim;
        int stepEasy = (othersize - remainT) / taskDim;
        int stepHard = stepEasy + 1;
        int step = (taskId < remainT ? stepHard : stepEasy);
        int indStart = (taskId < remainT ? taskId * stepHard : (taskId - remainT) * stepEasy + remainT * stepHard);

        for (int i = indStart; i < indStart + step; i++) {

            int indd = 0;
            int indi = i;
            int lastI = indi % middle;

            indd += (indi % middle) * strideD_m;
            indi /= middle;

            indd += (indi % startDim) * strideD_f;

            if (mask + 1 + lastI < maxNum) {
                __bang_write_value(src, maxNum, -INFINITY);                                   //提前设置负无穷
                __memcpy(src, destination + indd, (mask + 1 + lastI) * sizeof(T), GDRAM2NRAM);//从destination读取对应数据
                __bang_argmax(srcMax, src, maxNum);                                           //获取最大值
                __bang_write_value(destSum, maxNum, srcMax[0]);
                __memcpy(destSum, src, (mask + 1 + lastI) * sizeof(T), NRAM2NRAM);//destSum前面(mask + 1 + lastI)为src，后面部分为最大值
                __bang_sub_scalar(destSum, destSum, srcMax[0], maxNum);           //destSum前面(mask + 1 + lastI)为(src - M)，后面部分为0
                __bang_active_exp_less_0(destSum, destSum, maxNum);               //destSum前面(mask + 1 + lastI)为exp(src - M)，后面部分为1
                __bang_write_zero(src, maxNum);                                   //重新设置src全部为0
                __memcpy(src, destSum, (mask + 1 + lastI) * sizeof(T), NRAM2NRAM);//src前面(mask + 1 + lastI)为exp(src - M)，后面部分为0

                if (maxNum >= wSize) {
                    int segNum = maxNum / wSize;//准备数值求和
                    for (int strip = segNum / 2; strip > 0; strip = strip / 2) {
                        for (int j = 0; j < strip; j++) {
                            __bang_add(destSum + j * wSize, destSum + j * wSize, destSum + (j + strip) * wSize, wSize);
                        }
                    }
                    __bang_reduce_sum(destSumFinal, destSum, wSize);//此时destSum[0]保存的就是当前maxNum长度数据的数值和

                } else {
                    __memcpy(destSumFinal, destSum, maxNum * sizeof(T), NRAM2NRAM);
                    __bang_reduce_sum(destSumFinal, destSumFinal, wSize);//此时destSum[0]保存的就是当前maxNum长度数据的数值和
                }
                T globalSumInv = 1.0 / (destSumFinal[0] - (maxNum - (mask + 1 + lastI)));//下面开始指数变换，写回GDRAM
                __bang_mul_scalar(src, src, globalSumInv, maxNum);

                __memcpy(destination + indd, src, maxNum * sizeof(T), NRAM2GDRAM);
                __bang_write_zero(src, maxNum);
                for (int s = 1; s < repeat; s++) {
                    __memcpy(destination + indd + s * maxNum, src, maxNum * sizeof(T), NRAM2GDRAM);
                }
                if (remain) {
                    __memcpy(destination + indd + repeat * maxNum, src, remain * sizeof(T), NRAM2GDRAM);
                }
            } else {
                int newRemain = (mask + 1 + lastI) % maxNum;
                int nR = (mask + 1 + lastI - newRemain) / maxNum;

                __bang_write_zero(destSum, maxNum);
                __bang_write_zero(destSumFinal, wSize);

                destOldMax = -INFINITY;
                destNewMax = -INFINITY;
                for (int s = 0; s < nR; s++) {

                    __memcpy(src, destination + indd + s * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                    __bang_argmax(srcMax, src, maxNum);

                    if (destNewMax < srcMax[0]) {
                        destNewMax = srcMax[0];
                    }
                    __bang_sub_scalar(src, src, destNewMax, maxNum);
                    __bang_active_exp_less_0(src, src, maxNum);

                    if (s > 0) {
                        __bang_mul_scalar(destSum, destSum, exp(destOldMax - destNewMax), maxNum);
                    }
                    __bang_add(destSum, destSum, src, maxNum);

                    destOldMax = destNewMax;
                }

                if (newRemain) {
                    //__bang_write_value(src, maxNum, -INFINITY);

                    __memcpy(src, destination + indd + nR * maxNum, newRemain * sizeof(T), GDRAM2NRAM);

                    __bang_argmax(srcMax, src, maxNum);

                    if (destNewMax < srcMax[0]) {
                        destNewMax = srcMax[0];
                    }

                    __bang_write_value(tmp, maxNum, destNewMax);
                    __memcpy(tmp, src, newRemain * sizeof(T), NRAM2NRAM);

                    __bang_sub_scalar(tmp, tmp, destNewMax, maxNum);
                    __bang_active_exp_less_0(tmp, tmp, maxNum);

                    if (nR > 0) {
                        __bang_mul_scalar(destSum, destSum, exp(destOldMax - destNewMax), maxNum);
                    }
                    __bang_add(destSum, destSum, tmp, maxNum);

                    destOldMax = destNewMax;
                }

                if (maxNum >= wSize) {
                    int segNum = maxNum / wSize;//准备数值求和
                    for (int strip = segNum / 2; strip > 0; strip = strip / 2) {
                        for (int j = 0; j < strip; j++) {
                            __bang_add(destSum + j * wSize, destSum + j * wSize, destSum + (j + strip) * wSize, wSize);
                        }
                    }
                    __bang_reduce_sum(destSumFinal, destSum, wSize);//此时destSum[0]保存的就是当前maxNum长度数据的数值和

                } else {

                    __memcpy(destSumFinal, destSum, maxNum * sizeof(T), NRAM2NRAM);
                    __bang_reduce_sum(destSumFinal, destSumFinal, wSize);//此时destSum[0]保存的就是当前maxNum长度数据的数值和
                }

                T globalSumInv;
                if (newRemain) {
                    globalSumInv = 1.0 / (destSumFinal[0] - (maxNum - newRemain));//下面开始指数变换，写回GDRAM

                } else {
                    globalSumInv = 1.0 / destSumFinal[0];//下面开始指数变换，写回GDRAM
                }

                for (int s = 0; s < nR; s++) {
                    __memcpy(src, destination + indd + s * maxNum, maxNum * sizeof(T), GDRAM2NRAM);

                    __bang_sub_scalar(src, src, destNewMax, maxNum);
                    __bang_active_exp_less_0(src, src, maxNum);
                    __bang_mul_scalar(src, src, globalSumInv, maxNum);

                    __memcpy(destination + indd + s * maxNum, src, maxNum * sizeof(T), NRAM2GDRAM);
                }
                __bang_write_zero(src, maxNum);
                for (int s = nR; s < repeat; s++) {
                    __memcpy(destination + indd + s * maxNum, src, maxNum * sizeof(T), NRAM2GDRAM);
                }
                if (remain) {
                    __memcpy(destination + indd + repeat * maxNum, src, remain * sizeof(T), NRAM2GDRAM);
                }

                if (newRemain) {

                    __memcpy(src, destination + indd + nR * maxNum, newRemain * sizeof(T), GDRAM2NRAM);

                    __bang_sub_scalar(src, src, destNewMax, maxNum);
                    __bang_active_exp_less_0(src, src, maxNum);
                    __bang_mul_scalar(src, src, globalSumInv, maxNum);

                    __memcpy(destination + indd + nR * maxNum, src, newRemain * sizeof(T), NRAM2GDRAM);
                }
            }
        }
    } else {
        T *src = (T *) nram_buffer;      //[dimS]
        T *destSum = src + dimS;         //[dimS]
        T *destSumFinal = destSum + dimS;//[wSize]

        int remainT = othersize % taskDim;
        int stepEasy = (othersize - remainT) / taskDim;
        int stepHard = stepEasy + 1;
        int step = (taskId < remainT ? stepHard : stepEasy);
        int indStart = (taskId < remainT ? taskId * stepHard : (taskId - remainT) * stepEasy + remainT * stepHard);

        for (int i = indStart; i < indStart + step; i++) {

            int indd = 0;
            int indi = i;


            indd += (indi % middle) * strideD_m;
            indi /= middle;

            indd += (indi % startDim) * strideD_f;
            __bang_write_value(src, dimS, -INFINITY);
            __bang_write_zero(destSumFinal, wSize);
            int lastI = i % middle;
            __memcpy(src, destination + indd, (mask + 1 + lastI) * sizeof(T), GDRAM2NRAM);
            __bang_argmax(srcMax, src, dimS);
            __bang_write_value(destSum, dimS, srcMax[0]);
            __memcpy(destSum, src, (mask + 1 + lastI) * sizeof(T), NRAM2NRAM);
            __bang_sub_scalar(destSum, destSum, srcMax[0], dimS);
            __bang_active_exp_less_0(destSum, destSum, dimS);
            __bang_write_zero(src, dimS);
            __memcpy(src, destSum, (mask + 1 + lastI) * sizeof(T), NRAM2NRAM);
            int segNum = dimS / wSize;//准备数值求和
            for (int strip = segNum / 2; strip > 0; strip = strip / 2) {
                for (int j = 0; j < strip; j++) {
                    __bang_add(destSum + j * wSize, destSum + j * wSize, destSum + (j + strip) * wSize, wSize);
                }
            }
            __bang_reduce_sum(destSumFinal, destSum, wSize);                       //此时destSum[0]保存的就是当前maxNum长度数据的数值和
            T globalSumInv = 1.0 / (destSumFinal[0] - (dimS - (mask + 1 + lastI)));//下面开始指数变换，写回GDRAM
            __bang_mul_scalar(src, src, globalSumInv, dimS);

            __memcpy(destination + indd, src, dimsize * sizeof(T), NRAM2GDRAM);
        }
    }
}

template<typename T>
void causal_softmaxUnionDim_3(cnrtQueue_t queue, void *destination, int strideD_f, int strideD_m, int othersize, int middle, int dimsize, int mask) {
    int wSize = 128 / sizeof(T);
    auto y_ = reinterpret_cast<T *>(destination);

    int dimS;
    float mi = log2(dimsize);
    if (floor(mi) == mi) {
        dimS = dimsize;
    } else {
        dimS = pow(2, floor(mi) + 1);
    }
    if (dimS < wSize) {
        dimS = wSize;
    }

    cnrtDim3_t k_dim;
    cnrtFunctionType_t k_type;

    k_dim.x = 4;
    k_dim.y = 1;
    k_dim.z = 1;
    k_type = CNRT_FUNC_TYPE_UNION1;

    causal_softmaxDim_3<T><<<k_dim, k_type, queue>>>(y_, strideD_f, strideD_m, othersize, middle, dimsize, dimS, mask);
    cnrtQueueSync(queue);
}

void causal_softmax_bang_f16(CausalSoftmaxBangDescriptor_t desc, void *workspace, void *y, void *stream) {
    int n = desc->n;
    int d = desc->shape[desc->ndim - 1];
    int mask = desc->shape[desc->ndim - 1] - desc->shape[desc->ndim - 2];
    auto queue = reinterpret_cast<cnrtQueue_t>(stream);

    if (desc->ndim == 2) {
        int strideD_f = desc->stride[0];
        causal_softmaxUnionDim_2<half>(queue, y, strideD_f, n, d, mask);

    } else if (desc->ndim == 3) {
        int strideD_f = desc->stride[0];
        int strideD_m = desc->stride[1];
        int middle = desc->shape[1];
        causal_softmaxUnionDim_3<half>(queue, y, strideD_f, strideD_m, n, middle, d, mask);

    } else {
        int *mlu_strideY = reinterpret_cast<int *>(workspace);
        int *mlu_shape = mlu_strideY + desc->ndim;

        CNRT_CHECK(cnrtMemcpy(mlu_strideY, desc->stride, desc->ndim * sizeof(int), cnrtMemcpyHostToDev));
        CNRT_CHECK(cnrtMemcpy(mlu_shape, desc->shape, desc->ndim * sizeof(int), cnrtMemcpyHostToDev));

        causal_softmax<half>(queue, y, mlu_strideY, mlu_shape, n, d, mask, desc->ndim);
    }
}

infiniopStatus_t bangCausalSoftmax(CausalSoftmaxBangDescriptor_t desc,
                                   void *workspace,
                                   unsigned long int workspace_size,
                                   void *data,
                                   void *stream) {
    if (dtype_eq(desc->dtype, F16)) {
        causal_softmax_bang_f16(desc, workspace, data, stream);
        return STATUS_SUCCESS;
    }
    return STATUS_BAD_TENSOR_DTYPE;
}